model:
  - tabNTL
network:
  - tabNTL
trainer:
  - my
device:
  - cuda
batch_size:
  - 1000
learning_rate:
  - 0.001
training_epochs:
  - 300
enc_nlayers:
  - 5
num_trans:
  - 11
trans_nlayers:
  - 2
trans_type:
  - residual
loss:
  - DCL
enc_bias:
  - True
batch_norm:
  - False
loss_temp:
  - 0.1
l2:
  - 0.0
optimizer:
  - Adam
scheduler:
  -
    class: StepLR
    args:
      step_size: 200
      gamma: 0.5
#early_stopper:
#  -
#    class: Patience
#    args:
#      patience: 100
#      use_train_loss: True

shuffle:
  - True
num_repeat:
  - 1
save_scores:
  - False
result_folder:
  - RESULTS/RESULTS_